{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 01: Simple Perceptron Learning Method\n",
    "\n",
    "Name: Kathiravan Natarajan\n",
    "\n",
    "cwid: 50174332\n",
    "\n",
    "Due: **Wednesday February 22, 2017**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy as sp\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ensure visualizations appear inline inside of notebooks\n",
    "# and set default figure size to 10x10 in (modify this to optimize for your display)\n",
    "%matplotlib inline\n",
    "import pylab\n",
    "pylab.rcParams['figure.figsize'] = (12.0, 5.0) # 12 inches x 5 inches\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1: Load dataset\n",
    "\n",
    "Load data from dataset1 csv file, define X_nobias input numpy array and y output array.\n",
    "\n",
    "You should find a file in your repository assignments directory named `dataset1.csv`.  This file is a comma\n",
    "separated format.  The first line label the columns of the dataset, where the first 2 columns `x_1`, `x_2`\n",
    "are inputs for a binary classification task, and the third column, `y` is the binary class, 0 for negative\n",
    "classification and 1 for positive classification.\n",
    "\n",
    "In the next cell, read in the data set from the `dataset1.csv` file.  There are several ways to perform this task,\n",
    "for example one way is to use the `pandas` `read_csv()` function as we have done in the past in our lecture notebooks.\n",
    "\n",
    "Once you have read in the file, split the data in regular NumPy arrays named `X_nobias` and `y`, where `X_nobias`\n",
    "should be an 8x2 shaped array containing the `x_1` and `x_2` columns from the data set, and `y` will be a\n",
    "numpy array of shape 8, (e.g. a vector of 8 items).  Make sure you convert these to regular NumpyArrays.  \n",
    "\n",
    "If you read in the arrays correctly, you should end up with the following resulting data structures:\n",
    "\n",
    "```python\n",
    ">>> print X_nobias.shape\n",
    "(8, 2)\n",
    "\n",
    ">>> print type(X_nobias)\n",
    "<type 'numpy.ndarray'>\n",
    "\n",
    ">>> print X_nobias\n",
    "[[-0.80857143  0.8372093 ]\n",
    " [ 0.35714286  0.85049834]\n",
    " [-0.75142857 -0.73089701]\n",
    " [-0.3         0.12624585]\n",
    " [ 0.87142857  0.62458472]\n",
    " [-0.02       -0.92358804]\n",
    " [ 0.36285714 -0.31893688]\n",
    " [ 0.88857143 -0.87043189]]\n",
    " \n",
    " >>> print y.shape\n",
    "(8,)\n",
    "\n",
    ">>> print type(y)\n",
    "<type 'numpy.ndarray'>\n",
    "\n",
    ">>> print y\n",
    "[ 0.  0.  0.  0.  1.  1.  1.  1.]\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8L, 2L)\n",
      "<type 'numpy.ndarray'>\n",
      "(8L, 2L)\n",
      "(8L,)\n",
      "<type 'numpy.ndarray'>\n",
      "[ 0.  0.  0.  0.  1.  1.  1.  1.]\n"
     ]
    }
   ],
   "source": [
    "# load in the dataset1.csv file here\n",
    "\n",
    "data = pd.read_csv('dataset1.csv')\n",
    "\n",
    "\n",
    "# split the data in X_nobias numpy array with the x_1 and x_2 input columns, and y numpy array of the output\n",
    "# categories\n",
    "\n",
    "X_nobias = data[['x_1','x_2']].as_matrix()\n",
    "y = data['y'].as_matrix()\n",
    "print X_nobias.shape\n",
    "print type(X_nobias)\n",
    "print X_nobias.shape\n",
    "print y.shape\n",
    "print type(y)\n",
    "print y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2: Add Bias Column to Inputs\n",
    "\n",
    "Write a function that takes a numpy array of shape m rows x n columns and returns\n",
    "a new array of shape m+1 x n  and adds a column of 1's as the first column.\n",
    "\n",
    "As a warm up, and to get some practice with writing Python functions and the `NumPy` library, write a simple\n",
    "function that takes a mxn shaped numpy array and returns a new array with a 0th column of ones added which will\n",
    "be suitable to be used as bias inputs for the perceptron learning.\n",
    "\n",
    "Write your function in the following cell.  If you function is working correctly, you should be able to get\n",
    "the following results from adding a bias column to the X array read in in task 1.\n",
    "\n",
    "```python\n",
    ">>> X = add_bias_column(X_nobias)\n",
    ">>> print X.shape\n",
    "(8, 3)\n",
    ">>> print X\n",
    "[[ 1.         -0.80857143  0.8372093 ]\n",
    " [ 1.          0.35714286  0.85049834]\n",
    " [ 1.         -0.75142857 -0.73089701]\n",
    " [ 1.         -0.3         0.12624585]\n",
    " [ 1.          0.87142857  0.62458472]\n",
    " [ 1.         -0.02       -0.92358804]\n",
    " [ 1.          0.36285714 -0.31893688]\n",
    " [ 1.          0.88857143 -0.87043189]]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8L, 3L)\n",
      "[[ 1.         -0.80857143  0.8372093 ]\n",
      " [ 1.          0.35714286  0.85049834]\n",
      " [ 1.         -0.75142857 -0.73089701]\n",
      " [ 1.         -0.3         0.12624585]\n",
      " [ 1.          0.87142857  0.62458472]\n",
      " [ 1.         -0.02       -0.92358804]\n",
      " [ 1.          0.36285714 -0.31893688]\n",
      " [ 1.          0.88857143 -0.87043189]]\n"
     ]
    }
   ],
   "source": [
    "def add_bias_column(X_nobias):\n",
    "    \"\"\"Add a 0th column of bias inputs to the array of X inputs.  Return a new\n",
    "    array of inputs with this additional column of bias terms.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X_nobias - A m rows x n columns numpy array of inputs\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    X_bias - Returns a m+1 rows x n columns numpy array of inputs where the 0th column\n",
    "        X_bias[:,0] is a column of 1 values, and columns [1:] are the original inputs from\n",
    "        the X_nobias array columns [0:]\n",
    "    \"\"\"\n",
    "    # implement your solution here\n",
    "    X_ones = np.array(np.ones(8))\n",
    "    X_bias = np.column_stack([X_ones,X_nobias]) \n",
    "    #np.concatenate([np.array(x)[:,None],np.array(y)],axis=1)\n",
    "    # X_bias = np.concatenate([X_ones[:,0],X_nobias]) need to work more on this \n",
    "    return X_bias # return the new X_bias array you create\n",
    "\n",
    "X = add_bias_column(X_nobias)\n",
    "print X.shape\n",
    "print X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3: Split Inputs by Category\n",
    "\n",
    "Create another helper function to split the inputs up by the category.  \n",
    "\n",
    "In this assignment we are performing a binary\n",
    "classification task.  The `y` array has the correct categories.  To help us later on it will be useful to be able\n",
    "to split the inputs up into those inputs for the positive category (where `y` output = 1) and the negative\n",
    "category (where `y` output = 0).  \n",
    "\n",
    "**Hint**: You certainly can do this task using explicit loops (and feel free to do it that way)  but the\n",
    "task is realatively simple using NumPy fancy indexing (e.g. think of the values in `y` as an index that select\n",
    "the particular inputs in `X` when `y == 0` and when `y == 1`that you want).  If you are having problems using `numpy` vectorized operations here or in\n",
    "the rest of the assignment, try first programming the answer using explicit loops.\n",
    "\n",
    "In the following cell implement the given function named `split_inputs_by_category()`.  This function should\n",
    "take an input array X and a correct category output array y of corresponding outputs.  This function returns\n",
    "2 separate and new input arrays, `X_neg` and `X_pos` containing only the negative inputs and positive inputs\n",
    "respectively.\n",
    "\n",
    "**Hint 2**: You can return multiple values from a python function by simply return a list or tuple of\n",
    "the values.  For example, if you have 2 result variables names `result1` and `result2` you can return them both\n",
    "from a python function using a single return statement\n",
    "\n",
    "```python\n",
    "    return result1, result2\n",
    "```\n",
    "\n",
    "If you implement the function correctly, you should get the following results when using it:\n",
    "\n",
    "```python\n",
    ">>> X_neg, X_pos = split_inputs_by_category(X, y)\n",
    ">>> print X_neg\n",
    "[[ 1.         -0.80857143  0.8372093 ]\n",
    " [ 1.          0.35714286  0.85049834]\n",
    " [ 1.         -0.75142857 -0.73089701]\n",
    " [ 1.         -0.3         0.12624585]]\n",
    ">>> print X_pos\n",
    "[[ 1.          0.87142857  0.62458472]\n",
    " [ 1.         -0.02       -0.92358804]\n",
    " [ 1.          0.36285714 -0.31893688]\n",
    " [ 1.          0.88857143 -0.87043189]]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.         -0.80857143  0.8372093 ]\n",
      " [ 1.          0.35714286  0.85049834]\n",
      " [ 1.         -0.75142857 -0.73089701]\n",
      " [ 1.         -0.3         0.12624585]]\n",
      "[[ 1.          0.87142857  0.62458472]\n",
      " [ 1.         -0.02       -0.92358804]\n",
      " [ 1.          0.36285714 -0.31893688]\n",
      " [ 1.          0.88857143 -0.87043189]]\n",
      "(4L, 3L)\n",
      "[ 0.  0.  0.  0.  1.  1.  1.  1.]\n"
     ]
    }
   ],
   "source": [
    "def split_inputs_by_category(X, y):\n",
    "    \"\"\"This function splits up the X input array into 2 arrays.  All rows in X that are\n",
    "    in the negative category are returned as X_neg, and all input rows that are in the\n",
    "    positive category are returned as X_pos.  The categories are indicated by the y vector\n",
    "    that should be a vector of the same size as the number of rows/inputs in the X array.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X - A m rows of inputs by n columns numpy array.\n",
    "    y - A (m,) shaped numpy vector of binary categorical outputs\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    X_neg, X_pos - Returns numpy arrays of only those input rows where y==0 in X (X_neg)   \n",
    "       and where y==1 in X (X_pos)\n",
    "    \"\"\"\n",
    "    # Implement you solution here\n",
    "    '''\n",
    "    X_pos = np.array([np.array([X[i]  for i in range(8)])[y[:8].astype('bool')]])\n",
    "    y_n = np.logical_not(y)\n",
    "    X_neg = np.array([np.array([X[i]  for i in range(8)])[y_n[:8]]])\n",
    "    '''\n",
    "    X_pos = np.array(X[np.where(y>0)])\n",
    "    X_neg = np.array(X[np.where(y<=0)])\n",
    "    return X_neg, X_pos # example of returning a list or tuple of values\n",
    "\n",
    "X_neg, X_pos = split_inputs_by_category(X, y)\n",
    "print X_neg\n",
    "print X_pos\n",
    "print X_pos.shape\n",
    "print y "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 4: Implement Perceptron Weighted Sum\n",
    "\n",
    "A perceptron (as with all neural network units) simply starts out ising the weighted sum of its inputs, e.g.\n",
    "multiply each input by its corresponding weight and sum this result up.  Write a function that takes a current\n",
    "set of parameters $\\theta$ (theta) and a set of inputs `X` (m rows of input by n columns) and returns the weighted sum for each input\n",
    "\n",
    "**Hint** Again you can do this as an explcit loop, but there is also a simple `numpy` vectorzied way of calculating\n",
    "the weighted sum in a single operation.\n",
    "\n",
    "**Hint 2** If you used `numpy` vectorized operations you might end up with a (mx1) shaped column vector.  However it is much easier in later functions to assume you simply have a (m,) shaped vector of the resulting weighted sum\n",
    "of the inputs.  You can use the `x.flatten()` function on a number array to turn a (mx1) column vector back into a\n",
    "(m,) shaped array.\n",
    "\n",
    "If you implement the function correctly, you should get the following results when using it:\n",
    "\n",
    "```python\n",
    ">>> theta = np.array([[ 0.77187205],\n",
    "                      [-0.62170147],\n",
    "                      [ 0.76091527],\n",
    "                     ])\n",
    "\n",
    ">>> s = calc_weighted_sum(theta, X)\n",
    ">>> print s\n",
    "[ 1.91160744  1.19699298  0.6828856   1.05444488  0.70535968  0.08153384\n",
    "  0.30359929 -0.44287903]\n",
    "\n",
    ">>> print s.shape\n",
    "(8,)\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.91160744  1.19699298  0.6828856   1.05444488  0.70535968  0.08153384\n",
      "  0.30359929 -0.44287903]\n",
      "(8L,)\n"
     ]
    }
   ],
   "source": [
    "def calc_weighted_sum(theta, X): \n",
    "    \"\"\"This function returns the weighted sum for each input in X given the mode weight/parameters\n",
    "    in theta.  X should be an m rows of inputs x n columns shaped matrix, and theta should be a \n",
    "    nx1 column matrix of parameter values.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    theta - A n x 1 shaped column matrix of weight parameters.  If a bias term is being used, a bias\n",
    "       column of 1's should be added to the X inputs before calling this function, but in any case if\n",
    "       there are m inputs by n features in X, there need to be n theta parameters to calculate the \n",
    "       weighted sum.\n",
    "    X - A m rows of input by n columns of features numpy array.  Each row represents a separate input.\n",
    "       Each columns represents a feature of the input.  If we are representing a bias term in our models,\n",
    "       a column of 1's for the bias term needs to be added to X before calling this function.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    sum - Returns a (m,) shaped numpy vector of weighted sums, the result of applying the theta parameter\n",
    "       model to each of the m inputs in X.\n",
    "    \"\"\"\n",
    "    # Implement your solution here\n",
    "    \n",
    "    s = np.dot(X,theta).flatten()\n",
    "    return s\n",
    "  \n",
    "\n",
    "theta = np.array([[ 0.77187205],\n",
    "                  [-0.62170147],\n",
    "                  [ 0.76091527],\n",
    "                 ])\n",
    "\n",
    "s = calc_weighted_sum(theta, X)\n",
    "print s\n",
    "print s.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 5: Calculate Binary Threshold Output\n",
    "\n",
    "A perceptron does a simple binary threshold on the weighted sum hypothesis.  Using the function\n",
    "from previous task (`calc_weighted_sum`), calculate the perceptron output using a binary\n",
    "threshold at 0 of the weighted sum values.\n",
    "\n",
    "**Hint**: you can do this using a loop, but the np.where() function makes it easy to perform the binary\n",
    "threshold for this task.\n",
    "\n",
    "If you implement this function correctly, you should get the following results:\n",
    "\n",
    "```python\n",
    ">>> theta = np.array([[ 0.77187205],\n",
    "                      [-0.62170147],\n",
    "                      [ 0.76091527],\n",
    "                     ])\n",
    "\n",
    ">>> hypothesis = calc_perceptron_output(theta, X)\n",
    ">>> print hypothesis\n",
    "[ 1.  1.  1.  1.  1.  1.  1.  0.]\n",
    "\n",
    ">>> print hypothesis.shape\n",
    "(8,)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.  1.  1.  1.  1.  1.  1.  0.]\n",
      "(8L,)\n"
     ]
    }
   ],
   "source": [
    "def calc_perceptron_output(theta, X):\n",
    "    \"\"\"Calculate the output of the preceptron unit given current weights in theta for all input\n",
    "    values in X.  Use the weighted sum function from before to calculated the weighted sum of the\n",
    "    parameters.  This function returns an array of binary categories 0/1 which is the hypothesized\n",
    "    output for each input\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    theta - A n x 1 shaped column matrix of weight parameters.  If a bias term is being used, a bias\n",
    "       column of 1's should be added to the X inputs before calling this function, but in any case if\n",
    "       there are m inputs by n features in X, there need to be n theta parameters to calculate the \n",
    "       weighted sum.\n",
    "    X - A m rows of input by n columns of features numpy array.  Each row represents a separate input.\n",
    "       Each columns represents a feature of the input.  If we are representing a bias term in our models,\n",
    "       a column of 1's for the bias term needs to be added to X before calling this function.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    hypothesis - Returns a (m,) shaped numpy vector of binary categorical outputs, 0 for the negative\n",
    "       category and 1 for the positive category.\n",
    "    \"\"\"\n",
    "    # implement you solution here\n",
    "    s = np.dot(X,theta)\n",
    "    d = np.where(s >=0, 1.0, 0).flatten() \n",
    "    return d # return the calculated hypothesized categories as a (m,) shaped flattened vector\n",
    "\n",
    "\n",
    "theta = np.array([[ 0.77187205],\n",
    "                  [-0.62170147],\n",
    "                  [ 0.76091527],\n",
    "                 ])\n",
    "\n",
    "hypothesis = calc_perceptron_output(theta, X)\n",
    "print hypothesis\n",
    "print hypothesis.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 6: Split Input positive/negative and correct/incorrect\n",
    "\n",
    "The previous functions will allow us to calculate the hypothesized output given the current\n",
    "theta weight parameters of the perceptron.  A useful function to have for us to monitor the perceptron learning\n",
    "is if we could have a function that takes the inputs `X`, calculates the proposed/hypothesised outputs `y_hypothesis`\n",
    "and then determine which inputs we got correct and which we got incorrect.  For ease of use later on this function\n",
    "will actually return 4 sets of results, the positive examples we got correct (`X_pos_correct`), the positive\n",
    "examples we missed (`X_pos_incorrect`), and the correct and incorrect for the negative examples\n",
    "(`X_neg_correct` and `X_neg_incorrect` respectively).  We will use our `calc_perceptron_output()` function to\n",
    "calculated the hypothesized outputs, and the `split_inputs_by_category()` which will allow us to split by positive\n",
    "and negative category and also by the correct and incorrect results using the hypothesized outputs.\n",
    "\n",
    "**Hint** The `split_inputs_by_category` allows you to split the inputs on the correct outputs `y` assuming you\n",
    "implemented it correctly.  But it should be equally able to split a set of inputs on a calculated `y_hypothesis`\n",
    "just as easily.\n",
    "\n",
    "If you implement this function correctly, you should get the following results:\n",
    "\n",
    "```python\n",
    ">>> X_neg_correct, X_neg_incorrect, X_pos_correct, X_pos_incorrect = calc_correct_incorrect_by_category(theta, X, y)\n",
    ">>> print \"Negative examples we got correct:\\n\", X_neg_correct\n",
    "Negative examples we got correct:\n",
    "[]\n",
    "\n",
    ">>> print \"\\nNegative examples incorrect:\\n\", X_neg_incorrect\n",
    "Negative examples incorrect:\n",
    "[[ 1.         -0.80857143  0.8372093 ]\n",
    " [ 1.          0.35714286  0.85049834]\n",
    " [ 1.         -0.75142857 -0.73089701]\n",
    " [ 1.         -0.3         0.12624585]]\n",
    "\n",
    ">>> print \"\\nPositive examples correct:\\n\", X_pos_correct\n",
    "Positive examples correct:\n",
    "[[ 1.          0.87142857  0.62458472]\n",
    " [ 1.         -0.02       -0.92358804]\n",
    " [ 1.          0.36285714 -0.31893688]]\n",
    "\n",
    ">>> print \"\\nPositive examples incorrect:\\n\", X_pos_incorrect\n",
    "Positive examples incorrect:\n",
    "[[ 1.          0.88857143 -0.87043189]]\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Negative examples we got correct:\n",
      "[]\n",
      "\n",
      "Negative examples incorrect:\n",
      "[[ 1.         -0.80857143  0.8372093 ]\n",
      " [ 1.          0.35714286  0.85049834]\n",
      " [ 1.         -0.75142857 -0.73089701]\n",
      " [ 1.         -0.3         0.12624585]]\n",
      "\n",
      "Positive examples correct:\n",
      "[[ 1.          0.87142857  0.62458472]\n",
      " [ 1.         -0.02       -0.92358804]\n",
      " [ 1.          0.36285714 -0.31893688]]\n",
      "\n",
      "Positive examples incorrect:\n",
      "[[ 1.          0.88857143 -0.87043189]]\n"
     ]
    }
   ],
   "source": [
    "def calc_correct_incorrect_by_category(theta, X, y):\n",
    "    \"\"\"Given a set of theta weights for our perceptron and the inputs X plus the correct outputs y, split\n",
    "    our data into 4 results and return X_neg_correct, X_neg_incorrect, X_pos_correct, X_pos_incorrect\n",
    "    which are the input data that fell into each of the 4 logical possibilities for the current perceptron\n",
    "    model.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    theta - A n x 1 shaped column matrix of weight parameters.  If a bias term is being used, a bias\n",
    "       column of 1's should be added to the X inputs before calling this function, but in any case if\n",
    "       there are m inputs by n features in X, there need to be n theta parameters to calculate the \n",
    "       weighted sum.\n",
    "    X - A m rows of input by n columns of features numpy array.  Each row represents a separate input.\n",
    "       Each columns represents a feature of the input.  If we are representing a bias term in our models,\n",
    "       a column of 1's for the bias term needs to be added to X before calling this function.\n",
    "    y - A (m,) shaped vector of correct categories/outputs for the m inputs in X\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    X_neg_correct, X_neg_incorrect, X_pos_correct, X_pos_incorrect - Returns numpy arrays of the X inputs, split\n",
    "       into the 4 logical posibilities of the inputs in the negative category that were correct and incorrect, and\n",
    "       the inputs in the positive category that were correct and incorrect respectively.\n",
    "    \"\"\"\n",
    "    # implement you solution here\n",
    "    X_neg, X_pos = split_inputs_by_category(X, y)\n",
    "    X_neg_correct,X_neg_incorrect = split_inputs_by_category(X_neg, hypothesis[:4])\n",
    "    X_pos_incorrect,X_pos_correct = split_inputs_by_category(X_pos, hypothesis[4:])\n",
    "    \n",
    "    return X_neg_correct, X_neg_incorrect, X_pos_correct, X_pos_incorrect # make sure you return the 4 separated results\n",
    "\n",
    "\n",
    "X_neg_correct, X_neg_incorrect, X_pos_correct, X_pos_incorrect = calc_correct_incorrect_by_category(theta, X, y)\n",
    "print \"Negative examples we got correct:\\n\", X_neg_correct\n",
    "print \"\\nNegative examples incorrect:\\n\", X_neg_incorrect\n",
    "print \"\\nPositive examples correct:\\n\", X_pos_correct\n",
    "print \"\\nPositive examples incorrect:\\n\", X_pos_incorrect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 7: Calculate Decision Boundary\n",
    "\n",
    "One more small task.  For this problem with two input features and a bias feature term, a set of 3\n",
    "weights or theta parameters represents the equation of a line that will define the decision boundary being made\n",
    "by our perceptron unit.  The equation of the line is given by:\n",
    "\n",
    "$$\n",
    "y = \\theta_0 + \\theta_1 x_1 + \\theta_2  x_2\n",
    "$$\n",
    "\n",
    "The decision boundary occurs where $y = 0$.  Setting the above equation to 0 and solving for either of the\n",
    "inputs will give us an equation that allows us to determine the decision boundary line.  For example, solving for\n",
    "$x_2$:\n",
    "\n",
    "$$\n",
    "x_2 = \\frac{\\theta_0 + \\theta_1 x_1}{- \\theta_2}\n",
    "$$\n",
    "\n",
    "This equation allows us to easily determine points on the decision boundary line for any given set of \n",
    "theta parameter weights $\\theta_0, \\theta_1, \\theta_2$.\n",
    "\n",
    "Write a helper function that takes an array of theta parameters, and an array of $x_1$ values and calculates\n",
    "the corresponding $x_2$ values for each $x_1$ defining the decision boundary.\n",
    "\n",
    "If you implement this function correctly, you should get the following results:\n",
    "\n",
    "```python\n",
    ">>> x_1 = np.array([-1.0, 1.0])\n",
    "\n",
    ">>> x_2 = calc_decision_boundary_points(theta, x_1)\n",
    "\n",
    ">>> print x_1\n",
    "[-1.  1.]\n",
    "\n",
    ">>> print x_2\n",
    "[ 1.79125121 -0.18035865]\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def calc_decision_boundary_points(theta, x_1):\n",
    "    \"\"\"Calculate corresponding x_2 location for each point in x_1 on the decision boundary defined\n",
    "    by the given set of theta parameters.  The theta parameters define a decision function of the form\n",
    "\n",
    "    y = theta[0] + theta[1] * x_1 + theta[2] * x_2\n",
    "    \n",
    "    This function returns points x_2 that correspond to points in x_1 when y=0\n",
    "    \n",
    "    x_2 = (theta[0] + theta[1] * x_1[:]) / - theta[2]\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    theta - A n x 1 shaped column matrix of weight parameters.  If a bias term is being used, a bias\n",
    "       column of 1's should be added to the X inputs before calling this function, but in any case if\n",
    "       there are m inputs by n features in X, there need to be n theta parameters to calculate the \n",
    "       weighted sum.\n",
    "     x_1 - A numpy array of x values to compute corresponding x_t values for the given set of theta parameters.\n",
    "        NOTE: This function assumes only 2 features, unlike previous function which should work generally for \n",
    "        any number of n features.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    x_2 - A numpy array the same size and shape as x_1 of the calculated second point on the decision boundary\n",
    "        associated with each point in x_1\n",
    "    \n",
    "    \"\"\"\n",
    "    # implememnt your solution here\n",
    "    return # make sure you return a numpy array the same shape as the input array x_1\n",
    "\n",
    "# for example, for the given set of theta we were using before, we can calculate pairs of points\n",
    "# on the decision boundary for when x_1 = -1.0 and 1.0 like this\n",
    "x_1 = np.array([-1.0, 1.0])\n",
    "x_2 = calc_decision_boundary_points(theta, x_1)\n",
    "print x_1\n",
    "print x_2\n",
    "plt.plot(x_1, x_2, 'b-')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizing Perceptron Learning\n",
    "\n",
    "The following is a helper function that plots the current state of the perceptron learning.  It helps\n",
    "us visualize the current decision boundary, how the perceptron is currently performing, and the history\n",
    "of its learning.  This function will be used to test your implementation of the perceptron learning\n",
    "method.  This function uses some of the functions you implemented above to produce outputs.  So if you \n",
    "implemented your function correctly above, you should be able to now run this cell and see the 8 data\n",
    "points in the dataset1 color coded by category and correct/incorrect, and the decision boundary\n",
    "indicated by theta.\n",
    "\n",
    "If your functions are implemented correctly, you will see a single figure from running the following cell.\n",
    "The left panel of the figure plots all of the inputs in the X input array.  The inputs are coded by shape\n",
    "according to their class, squares for positive classification and circles (0's) for negative classification.\n",
    "The inputs are further color coded by whether they were classified correctly by the indicated theta paramters,\n",
    "red for incorrect and green for correct.  The blue line represents the decision boundary that the current \n",
    "theta parameters give for our parameter space.  The yellow line represents the decision boundary for\n",
    "the given generously feasible vector.\n",
    "\n",
    "**NOTE**: You shouldn't have to do anything here, but if your functions above are incorrect this function may not work.\n",
    "\n",
    "** NOTE 2**: The theta_gen_feas represents a generously feasible (correct) solution.  We will talk about this\n",
    "in class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def visualize_perceptron_state(theta, theta_gen_feas, X, y, error_history, theta_distance_history):\n",
    "    \"\"\"We will visualize current state of perceptron learning given the set of inputs X and the current\n",
    "    values of the theta weight parameters of the system theta.  We need to know the correct output categories\n",
    "    y in order to visualize the correct/incorrect inputs\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    theta - A n x 1 shaped column matrix of weight parameters.  If a bias term is being used, a bias\n",
    "       column of 1's should be added to the X inputs before calling this function, but in any case if\n",
    "       there are m inputs by n features in X, there need to be n theta parameters to calculate the \n",
    "       weighted sum.\n",
    "    theta_gen_feas - A generously feasible solution vector of theta parameters.  This should be the same\n",
    "       shape as theta.  If provided, we plot the decision boundary specified by this generous solution.\n",
    "    X - A m rows of input by n columns of features numpy array.  Each row represents a separate input.\n",
    "       Each columns represents a feature of the input.  If we are representing a bias term in our models,\n",
    "       a column of 1's for the bias term needs to be added to X before calling this function.\n",
    "    y - A (m,) shaped vector of correct categories/outputs for the m inputs in X\n",
    "    error_history - A normal numpy list, contains the history of the absolute number of miscategorized\n",
    "       values for each epoch.  Must be provided and these values are plotted in the right panels showing\n",
    "       the history of training over the training epochs.\n",
    "    theta_distance_history - A normal numpy list, contains the history of the distances between the current\n",
    "       value of the theta paramters to the generously feasible solution vector.  If provided, we plot the\n",
    "       history of the distances on the second axis of the epochs/error-distance figure.\n",
    "    \"\"\"\n",
    "    ## top left panel, plot all input points, use shape to indicate category circle=negative, square=positive\n",
    "    # and use color to indicate result green=correct, red=incorrect.  Also indicate the location of the\n",
    "    # decision boundary determined by the theta parameters\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.title('Perceptron Classifier')\n",
    "    plt.xlabel(u'$x_1$')\n",
    "    plt.ylabel(u'$x_2$')\n",
    "    X_neg_correct, X_neg_incorrect, X_pos_correct, X_pos_incorrect = calc_correct_incorrect_by_category(theta, X, y)\n",
    "    plt.plot(X_neg_correct[:,1], X_neg_correct[:,2], 'go', markersize=8, label='Negative category (correct)')\n",
    "    plt.plot(X_neg_incorrect[:,1], X_neg_incorrect[:,2], 'ro', markersize=8, label='Negative category (incorrect)')\n",
    "    plt.plot(X_pos_correct[:,1], X_pos_correct[:,2], 'gs', markersize=8, label='Positive category (correct)')\n",
    "    plt.plot(X_pos_incorrect[:,1], X_pos_incorrect[:,2], 'rs', markersize=8, label='Positive category (incorrect)')\n",
    "    # now determine the decision boundary for two points of x_1 at -1.0 and 1.0\n",
    "    x_1 = np.array([-1.0, 1.0])\n",
    "    x_2 = calc_decision_boundary_points(theta, x_1)\n",
    "    plt.plot(x_1, x_2, 'b-', label='decision boundary')\n",
    "    # now determine decision boundry defined by the generously feasable vector\n",
    "    if not theta_gen_feas is None:\n",
    "        x_2 = calc_decision_boundary_points(theta_gen_feas, x_1)\n",
    "        plt.plot(x_1, x_2, 'y-', label='generously feasable decision boundary')\n",
    "    #plt.legend(loc='best', fontsize=6)\n",
    "    plt.axis([-1, 1, -1, 1])\n",
    "    \n",
    "    ## top right panel, show history of number of errors classifier makes over the iterations of\n",
    "    # learning for the perceptron classifier\n",
    "    ax1 = plt.subplot(1,2,2)\n",
    "    ax2 = ax1.twinx()\n",
    "    ax1.set_title('Number of Errors / Distance')\n",
    "    ax1.set_xlabel('Training Epoch / Iteration')\n",
    "    ax1.set_ylabel('Number of Errors')\n",
    "    epochs = len(error_history)\n",
    "    p1, = ax1.plot(range(epochs), error_history, 'k-', label='Errors')\n",
    "    if epochs < 10:\n",
    "        ax1.set_xlim([-0.5, 10])\n",
    "        \n",
    "    ## top right, show distance of classifier training as second Y axis\n",
    "    print theta_distance_history\n",
    "    if len(theta_distance_history) != 0:\n",
    "        ax2.set_ylabel('Distance')\n",
    "        p2, = ax2.plot(range(epochs), theta_distance_history, 'k--', label='Distance')\n",
    "        plt.legend([p1, p2], ['Errors', 'Distance'], loc='best', fontsize=10)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "\n",
    "\n",
    "theta = np.array([[ 0.77187205],\n",
    "                  [-0.62170147],\n",
    "                  [ 0.76091527],\n",
    "                 ])\n",
    "theta_gen_feas = np.array([[-0.69414749],\n",
    "                           [ 4.3496526 ],\n",
    "                           [-2.60997235],\n",
    "                          ])\n",
    "visualize_perceptron_state(theta, theta_gen_feas, X, y, [4, 3, 1, 1, 0], [8.0, 4.0, 2.2, 1.5, 1.0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 8: Perceptron Learning Rule\n",
    "\n",
    "**Task 8**: Here you will actually implement the perceptron learning rule.  You have been given the function\n",
    "and need to fill in the code at the indicated spots to implement learning.  This funciton iteratively updates\n",
    "the theta parameter weights until it gets all of the input classifications correct.  This function makes use\n",
    "of the previous function to visualize the results, and it is interactive so it pauses after each iteration to\n",
    "display the results (hit any key to perform the next iteration).\n",
    "\n",
    "First of all implement the `update_theta` learning rule for the perceptron we described in class.  Basically,\n",
    "given the current set of theta parameters, this function should find all of the positive category inputs that \n",
    "were misclassified, and add the input row for each misclassified item to the theta parameters.  Then it\n",
    "does the same with the misclassified negative category inputs, but it subtracts rather than adds the input\n",
    "of each of these to the theta parameters.  The resulting set of new theta parameters that is calculated is\n",
    "returned from this function.\n",
    "\n",
    "If you implement this function correctly, you should get the following outputs when using it:\n",
    "\n",
    "```python\n",
    ">>> theta = np.array([[ 0.77187205],\n",
    "                      [-0.62170147],\n",
    "                      [ 0.76091527],\n",
    "                     ])\n",
    "\n",
    ">>> theta_next = update_theta(theta, X, y)\n",
    ">>> print theta_next\n",
    "[[-2.22812795]\n",
    " [ 1.7697271 ]\n",
    " [-1.1925731 ]]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def update_theta(theta, X, y):\n",
    "    \"\"\"Implement perceptron learning rule.  This function returns a new set of theta weight parameters\n",
    "    after updating weights based on incorrectly classified inputs\n",
    "    \"\"\"\n",
    "    # make a copy of current theta weights, to retun as result\n",
    "    theta_updated = np.copy(theta)\n",
    "    \n",
    "    # implement your solution here\n",
    "    \n",
    "    return theta_updated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "theta = np.array([[-0.62170147],\n",
    "                  [ 0.76091527],\n",
    "                  [ 0.77187205]])\n",
    "\n",
    "theta_next = update_theta(theta, X, y)\n",
    "print theta_next"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Perceptron Learning**: Once the update_theta function is working the following function should iteratively\n",
    "run multiple epochs of training on the perceptron.  It will use your `update_theta()` and \n",
    "`calc_perceptron_output()` functions, which in turn your other smaller helper functions.  For dataset1, if you\n",
    "have correctly implemented all you helper functions for the perceptron, you should get the following output\n",
    "results when running with a given initial theta:\n",
    "\n",
    "```python\n",
    ">>> theta = np.array([[ 0.77187205],\n",
    "                      [-0.62170147],\n",
    "                      [ 0.76091527],\n",
    "                     ])\n",
    ">>> theta_gen_feas = np.array([[-0.69414749],\n",
    "                               [ 4.3496526 ],\n",
    "                               [-2.60997235],\n",
    "                              ])\n",
    "\n",
    ">>> learn_perceptron(theta, theta_gen_feas, X, y)\n",
    "Epoch: 0  number incorrect: 5\n",
    "\n",
    "     theta:  [[ 0.77187205]\n",
    " [-0.62170147]\n",
    " [ 0.76091527]]\n",
    " \n",
    "Epoch: 1  number incorrect: 3\n",
    "\n",
    "     theta:  [[-2.22812795]\n",
    " [ 1.7697271 ]\n",
    " [-1.1925731 ]]\n",
    " \n",
    "Epoch: 2  number incorrect: 1\n",
    "\n",
    "     theta:  [[ 0.77187205]\n",
    " [ 2.98401282]\n",
    " [-1.8105133 ]]\n",
    " \n",
    "Epoch: 3  number incorrect: 0\n",
    "\n",
    "     theta:  [[-0.22812795]\n",
    " [ 2.62686996]\n",
    " [-2.66101164]]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import euclidean\n",
    "from scipy.linalg import norm\n",
    "\n",
    "def learn_perceptron(theta, theta_gen_feas, X, y):\n",
    "    \"\"\"Learn the theta parameters for a perceptron for the given inputs X and correct binary categories\n",
    "    Y.  This function assumes that a bias column/feature has already been added to the inputs X and\n",
    "    that the size of theta matches the number of features with bias in the input.\n",
    "    This function iterates until the classifier gets all inputs correct and returns the final\n",
    "    set of theta weight parameters that achieve correct classification.\n",
    "    \"\"\"\n",
    "    # bookeeping\n",
    "    m, n = X.shape # m = number of inputs, n = number of \n",
    "    error_history = [] # used to keep track of errors made in each iteration\n",
    "    theta_dist_history = [] # used to keep track of distance metric on theta weights in each iteration\n",
    "    epoch = 0 # keep track of which epoch of training we are on\n",
    "    \n",
    "    # iterate while the classifier is still misclassifying some inputs\n",
    "    while True:\n",
    "        # calculate correct/incorrect classifications for this epoch\n",
    "        y_hypothesis = calc_perceptron_output(theta, X)\n",
    "        num_incorrect = np.sum(y != y_hypothesis)\n",
    "        error_history.append(num_incorrect)\n",
    "        \n",
    "        # keep track of distance to generously feasible vector\n",
    "        if not theta_gen_feas is None:\n",
    "            dist = euclidean(theta, theta_gen_feas)\n",
    "            #dist = norm(theta - theta_gen_feas)\n",
    "            theta_dist_history.append(dist)\n",
    "            \n",
    "        # visualize current state of the classifier after this epoch\n",
    "        print \"Epoch: %d  number incorrect: %d\\n\" % (epoch, num_incorrect)\n",
    "        print \"     theta: \", theta\n",
    "        fig = plt.figure(figsize=(12,6))\n",
    "        visualize_perceptron_state(theta, theta_gen_feas, X, y, error_history, theta_dist_history)\n",
    "        #key = raw_input('Press \"Enter\" to continue or \"q\" to quit...')\n",
    "        #if key == 'q':\n",
    "        #    break\n",
    "        \n",
    "        # update theta weight/parameters for this epoch\n",
    "        theta = update_theta(theta, X, y)\n",
    "        epoch += 1\n",
    "        \n",
    "        # stop iterating when we get all inputs correct or exceed some number of epochs of training\n",
    "        if num_incorrect == 0 or epoch > 10:\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "theta = np.array([[ 0.77187205],\n",
    "                  [-0.62170147],\n",
    "                  [ 0.76091527],\n",
    "                 ])\n",
    "theta_gen_feas = np.array([[-0.69414749],\n",
    "                           [ 4.3496526 ],\n",
    "                           [-2.60997235],\n",
    "                          ])\n",
    "\n",
    "learn_perceptron(theta, theta_gen_feas, X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Further Tasks\n",
    "\n",
    "Dataset 2 is not linearly separable.  To further test you functions, and see what happens to the perceptron\n",
    "when the data cannot be separated by a line, try loading and running the perceptron learning with dataset 2.\n",
    "\n",
    "In addition datasets 3 and 4 have also been provided with larger data sets.  Which of these data sets are linearly\n",
    "separable?  Which ones are not?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('dataset2.csv')\n",
    "\n",
    "X_nobias = df.ix[:,0:2].as_matrix()\n",
    "y = df.y.as_matrix()\n",
    "m,n = X_nobias.shape\n",
    "X = add_bias_column(X_nobias)\n",
    "print X\n",
    "print y\n",
    "print m\n",
    "print n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "theta = np.array([[-0.54178883],\n",
    "                  [ 1.84689887],\n",
    "                  [-0.58324929],\n",
    "                  ])\n",
    "theta_gen_feas = None\n",
    "learn_perceptron(theta, None, X, y)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
